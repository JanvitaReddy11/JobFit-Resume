{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "import datetime\n",
    "import subprocess\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('GROQ_API_KEY')\n",
    "client = Groq(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 15, 'total_tokens': 41, 'completion_time': 0.021666667, 'prompt_time': 0.002298616, 'queue_time': 0.011478854, 'total_time': 0.023965283}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-904989d9-c694-4aa3-8c1a-37f1b25a6757-0', usage_metadata={'input_tokens': 15, 'output_tokens': 26, 'total_tokens': 41})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm happy to help! However, I'm a large language model, I don't have the ability to know or remember your name. Each time you interact with me, it's a new conversation and I don't retain any information from previous conversations. So, I don't have your name to tell you. Would you like to introduce yourself?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 15, 'total_tokens': 86, 'completion_time': 0.059166667, 'prompt_time': 0.002920521, 'queue_time': 0.013163669000000001, 'total_time': 0.062087188}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-7e5cdcb8-9e74-4516-9c8a-144cecc21c3b-0', usage_metadata={'input_tokens': 15, 'output_tokens': 71, 'total_tokens': 86})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 40, 'total_tokens': 46, 'completion_time': 0.005, 'prompt_time': 0.013522623, 'queue_time': 0.113323887, 'total_time': 0.018522623}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-bdf86e84-f4b5-4b64-ba97-b340aa25d9f4-0', usage_metadata={'input_tokens': 40, 'output_tokens': 6, 'total_tokens': 46})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START,MessagesState, StateGraph\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Hi! I'm Bob.\", additional_kwargs={}, response_metadata={}, id='530c2991-8f45-41c8-98eb-dc7f69c6e35b'),\n",
       " AIMessage(content=\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 16, 'total_tokens': 42, 'completion_time': 0.021666667, 'prompt_time': 0.000153169, 'queue_time': 0.013667231, 'total_time': 0.021819836}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-76553beb-08fe-4016-8e0c-d57f33e43015-0', usage_metadata={'input_tokens': 16, 'output_tokens': 26, 'total_tokens': 42})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I remember! Your name is Bob!\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Hi! I'm Bob.\", additional_kwargs={}, response_metadata={}, id='530c2991-8f45-41c8-98eb-dc7f69c6e35b'),\n",
       " AIMessage(content=\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 16, 'total_tokens': 42, 'completion_time': 0.021666667, 'prompt_time': 0.000153169, 'queue_time': 0.013667231, 'total_time': 0.021819836}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-76553beb-08fe-4016-8e0c-d57f33e43015-0', usage_metadata={'input_tokens': 16, 'output_tokens': 26, 'total_tokens': 42}),\n",
       " HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='2c8eae58-beb5-405c-ae79-1d4cfd89995a'),\n",
       " AIMessage(content='I remember! Your name is Bob!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 56, 'total_tokens': 65, 'completion_time': 0.0075, 'prompt_time': 0.006951156, 'queue_time': 0.007915804, 'total_time': 0.014451156}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c4d9c192-de7a-480f-b013-9c7596736be0-0', usage_metadata={'input_tokens': 56, 'output_tokens': 9, 'total_tokens': 65})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert in answering questions related to user profile. You will be given with a resume in json and a job descriptipn. Your task is to answer the user related questions, to solve job queries\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm Jim.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = \"\"\"\n",
    " About the job\n",
    "\n",
    "Mercor is training models that predict job performance better than a human can. Our technology is so powerful that it’s trusted by customers ranging from startups making their first hires to the largest AI labs hiring thousands of experts to train LLMs.\n",
    "\n",
    "\n",
    "We’re at an 8-figure revenue run rate, growing 50% MoM, and have raised $32M from investors like Benchmark, General Catalyst, Peter Thiel, Jack Dorsey, Adam D'Angelo, and Larry Summers.\n",
    "\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "\n",
    "Regardless of whether you have human data experience, we think that successful Machine Learning Engineers at Mercor should have a solid technical background, demonstrated through:\n",
    "\n",
    "\n",
    "    2+ years of experience in back-end development (e.g., Python / Django)\n",
    "    Strong understanding of database design, SQL, and commonly used algorithms\n",
    "    Some experience with cloud platforms (GCP or AWS) and infrastructure management\n",
    "    Experience working with LLMs and/or other transformer-based models such as BERT\n",
    "    Experience with search or recommendation systems, data classification, or LTR models\n",
    "    Able to quickly understand complex problems & rapidly develop minimum viable solutions\n",
    "    Attention to detail, particularly in terms of producing high-quality, stable code that can support rapid scaling\n",
    "    A strong belief in regularly testing and using the things you build, and a strong sense of ownership for everything you’ve developed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../resume.json') as f:\n",
    "    resume_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START,MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"You are a highly skilled assistant specialized in answering questions related to user profiles, particularly resumes and job descriptions. \"\n",
    "         \"You will be provided with a resume (in JSON format) and a job description. Your task is to answer questions about the user's qualifications, \"\n",
    "         \"provide insights about how well the resume aligns with the job description. \"\n",
    "         \"You should be able to summarize the user's experience and answer questions in such a way that it aligns with the job description. \"\n",
    "         \"Chances for the user to get selected increases.\"\n",
    "        ),\n",
    "        # Placeholder for user and AI messages\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the workflow\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Model invocation function\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    # Assuming 'model' is your pre-defined model object, like OpenAI or another API\n",
    "    response = model.invoke(prompt)  # Replace with your actual model call\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Add workflow edges and nodes\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Memory management to save state during conversations\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the application with the memory checkpointer\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Assuming you have a defined model (e.g., OpenAI, Langchain model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "As I review the job description, I am excited to see how my skills and experience align with the requirements. With over 3 years of experience in back-end development, I have a solid technical background that would enable me to hit the ground running. My proficiency in Python, Django, and SQL would allow me to tackle complex problems and rapidly develop minimum viable solutions. Additionally, my experience working with various cloud platforms, including Azure and AWS, would enable me to effectively manage infrastructure. I'm particularly drawn to this role because of Mercor's innovative approach to training models that predict job performance better than humans can. My experience with machine learning models, including TensorFlow, Keras, and PyTorch, would be a strong asset in this role. Furthermore, my ability to quickly understand complex problems and produce high-quality, stable code that can support rapid scaling would be a valuable asset to the team. Overall, I believe my unique combination of technical skills, experience, and passion for innovation make me a strong fit for this role.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "threads = {}\n",
    "\n",
    "def handle_conversation(user_input, thread_id, resume=None, job_description=None):\n",
    "    if thread_id not in threads or resume or job_description:\n",
    "        # Initialize or reset the thread if needed\n",
    "        threads[thread_id] = {\n",
    "            \"resume\": resume,\n",
    "            \"job_description\": job_description,\n",
    "            \"messages\": [],\n",
    "        }\n",
    "\n",
    "        # Add the context to the conversation\n",
    "        context = []\n",
    "        if resume:\n",
    "            context.append(HumanMessage(content=f\"Here is the resume: {resume}\"))\n",
    "        if job_description:\n",
    "            context.append(HumanMessage(content=f\"Here is the job description: {job_description}\"))\n",
    "        \n",
    "        # Add the user input as the latest message\n",
    "        context.append(HumanMessage(content=user_input))\n",
    "    else:\n",
    "        # If thread exists and no new resume/job description, just add user input\n",
    "        context = [HumanMessage(content=user_input)]\n",
    "\n",
    "    # Assuming `app.invoke` is used to call the chatbot model (e.g., OpenAI API or another custom framework)\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    output = app.invoke({\"messages\": context}, config)\n",
    "    threads[thread_id][\"messages\"] = output[\"messages\"]\n",
    "    return output[\"messages\"][-1]\n",
    "\n",
    "# Example usage\n",
    "response = handle_conversation(\n",
    "    user_input=\"Describe in a para and respect to first person, why i am a good fit for this job\",\n",
    "    thread_id=\"thread_1\",\n",
    "    resume=resume_json,\n",
    "    job_description=job_description\n",
    ")\n",
    "\n",
    "# Assuming the output is a structured object with a `pretty_print()` method\n",
    "print(response.pretty_print())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As a Machine Learning Engineer with a strong technical background, I believe I would be a great fit for the role at Mercor. With my experience in back-end development, I am confident in my ability to design and implement scalable and efficient solutions. My understanding of database design, SQL, and algorithms would enable me to effectively manage and analyze large datasets. Additionally, my experience with cloud platforms and infrastructure management would allow', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 8111, 'total_tokens': 8193, 'completion_time': 0.068333333, 'prompt_time': 0.94821048, 'queue_time': 0.0017025290000000082, 'total_time': 1.016543813}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-b18b5224-603b-4404-bde7-a33e42a07b4e-0', usage_metadata={'input_tokens': 8111, 'output_tokens': 82, 'total_tokens': 8193})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
