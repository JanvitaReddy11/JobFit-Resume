{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = \"\"\"\n",
    " Opportunities with Episource, part of the Optum family of businesses. Join a premier provider of risk adjustment services, software and solutions that’s fueling innovation in the health care industry. Start a rewarding career where your work will empower health plans and medical groups with comprehensive end-to-end solutions designed to navigate health care efficiently. Our culture is rooted in innovation, encouraging our team to stay curious and engaged. By joining us, you become part of a global, remote/hybrid-friendly team dedicated to bridging health care gaps with a solid sense of social responsibility. At Episource, we are enriching lives, including those of our team members through Caring. Connecting. Growing together. \n",
    "\n",
    "As a Data Analyst, you will play a crucial role in transforming raw data into actionable insights that drive business decisions and strategies. Your responsibilities will include data collection, analysis, interpretation, and transformation, helping stakeholders understand complex datasets to make informed choices and optimize performance\n",
    "\n",
    "The data analyst will execute data pipelines, analyze transformed data to validate its completeness and gather insight to impact business decisions. Additionally, the data analysis will handle ad hoc analysis request-related data surrounding medical Record retrieval to assist in determining the location of medical charts.\n",
    "\n",
    "You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.\n",
    "\n",
    "Primary Responsibilities\n",
    "\n",
    "    Collect, clean, and preprocess data from various sources to ensure accuracy and consistency\n",
    "    Identify and address data quality issues, outliers, and missing values\n",
    "    Perform exploratory data analysis to uncover trends, patterns, and relationships within datasets\n",
    "    Oversee ingestion pipelines and troubleshoots errors related to data. Resolve errors using approved data cleaning techniques\n",
    "    Translate complex data analysis into clear and actionable insights for stakeholders\n",
    "    Collaborate with cross-functional teams to identify opportunities for process optimization and strategic decision-making\n",
    "    Build analytics tools that utilize the data pipeline to provide actionable insights into operational efficiency, and other critical business performance metrics\n",
    "    Develops and maintains solid relationships with all internal and external stakeholders\n",
    "    Document analysis methodologies, assumptions, and findings for future reference and replication\n",
    "    Requires an individual to maintain the ability to work in an environment with PHI / PII data \n",
    "\n",
    "You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.\n",
    "\n",
    "Required Qualifications\n",
    "\n",
    "    2+ years of experience in data analytics\n",
    "    2+ years of experience writing intermediate to advanced SQL (joining many tables, SQL functions, CTE's)\n",
    "    Experience working with Python (Ability to read and understand Python scripts and ability to make adjustments to scripts)\n",
    "    Experience with data visualization tools such as Tableau \n",
    "    Proven knowledge of data analysis and data quality best practices\n",
    "    Demonstrated excellent verbal and written communication skills, including ability to effectively communicate with internal and external customers\n",
    "    Proficient in Excel (v-look ups, pivot tables)\n",
    "    Ability and willingness to work occasional weekends and/or evening work\n",
    "\n",
    "Preferred Qualifications\n",
    "\n",
    "    Proven knowledge of healthcare data including member, claims, and provider data\n",
    "    Proven knowledge of industry standards and practices related to medical records retrieval\n",
    "    Demonstrated understanding of data pipelines, database structures and ETL practices \n",
    "    All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../resume.json') as f:\n",
    "    resume_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_exp = resume_json['work experience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "import datetime\n",
    "import subprocess\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('GROQ_API_KEY')\n",
    "client = Groq(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_exp(candidate_profile: dict, job_description: dict, max_trials: int = 5) -> None:\n",
    "    # Step 1: Define the prompt for optimizing the experience section\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert resume writer focused on optimizing profiles for ATS (Applicant Tracking System) compliance and job application success. Your task is to enhance the candidate's \"Experience\" section based on the provided job description:\n",
    "\n",
    "    1. **Extract Key Responsibilities and Achievements**:\n",
    "       - From the job description, identify and extract key responsibilities and metrics associated with the ideal candidate's role.\n",
    "\n",
    "    2. **Optimize the Candidate’s Experience Section**:\n",
    "       - Carefully review the \"Experience\" section and rewrite each bullet point to align with the job description and ATS compatibility.\n",
    "       - Follow the STAR method (Situation, Task, Action, Result) format to describe each experience.\n",
    "       - Use strong action verbs, quantify achievements if metrics are available, and prioritize relevance.\n",
    "       - Avoid repetition of action verbs and overuse of words.\n",
    "       - Ensure each bullet point aligns with the job description and highlights the candidate's relevant skills and achievements.\n",
    "\n",
    "    3. **Ethics and Integrity**:\n",
    "       - Do not invent new accomplishments or metrics.\n",
    "       - Keep the information truthful and concise.\n",
    "\n",
    "    4. **Output Format**:\n",
    "       - Provide the optimized \"Experience\" section in valid JSON format.\n",
    "       - The output must only contain the updated \"Experience\" section and nothing else.\n",
    "\n",
    "    ### Candidate Profile:\n",
    "    {json.dumps(candidate_profile, indent=4)}\n",
    "\n",
    "    ### Job Description:\n",
    "    {json.dumps(job_description, indent=4)}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Step 2: Query the LLM for optimization\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",  # Replace with your model name\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=2048,\n",
    "            top_p=0.9,\n",
    "            stream=False,\n",
    "            stop=None,\n",
    "        )\n",
    "\n",
    "        if not (completion.choices and completion.choices[0].message):\n",
    "            print(\"No valid response received from the LLM.\")\n",
    "            return\n",
    "\n",
    "        first_output = completion.choices[0].message.content\n",
    "\n",
    "        # Save the raw output to a file for reference\n",
    "        with open(\"experience_raw.txt\", \"w\") as f:\n",
    "            f.write(first_output)\n",
    "\n",
    "        # Step 3: Extract only the JSON object\n",
    "        second_prompt = f\"\"\"\n",
    "        You are a JSON parser. Extract only the valid JSON object from the following text without any additional explanation.\n",
    "\n",
    "        ### Input:\n",
    "        {first_output}\n",
    "\n",
    "        ### Output:\n",
    "        Provide only the JSON object.\n",
    "        \"\"\"\n",
    "\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[{\"role\": \"user\", \"content\": second_prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=1024,\n",
    "            top_p=0.9,\n",
    "            stream=False,\n",
    "            stop=None,\n",
    "        )\n",
    "\n",
    "        if not (second_response.choices and second_response.choices[0].message):\n",
    "            print(\"No valid response received for JSON extraction.\")\n",
    "            return\n",
    "\n",
    "        second_output = second_response.choices[0].message.content\n",
    "\n",
    "        # Attempt to save the JSON object, retrying up to max_trials\n",
    "        for attempt in range(max_trials):\n",
    "            try:\n",
    "                extracted_json = json.loads(second_output.strip())\n",
    "                with open(\"experience.json\", \"w\") as file:\n",
    "                    json.dump(extracted_json, file, indent=4)\n",
    "                print(\"Optimized experience JSON saved to 'experience.json'.\")\n",
    "                return\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Trial {attempt + 1}/{max_trials}: Error parsing JSON. Retrying...\")\n",
    "                time.sleep(1)  # Optional delay before retry\n",
    "\n",
    "        print(\"Failed to parse and save JSON after maximum retries.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LLM processing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized experience JSON saved to 'experience.json'.\n"
     ]
    }
   ],
   "source": [
    "analyze_exp(candidate_exp,job_description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
