{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = \"\"\"\n",
    " Opportunities with Episource, part of the Optum family of businesses. Join a premier provider of risk adjustment services, software and solutions that’s fueling innovation in the health care industry. Start a rewarding career where your work will empower health plans and medical groups with comprehensive end-to-end solutions designed to navigate health care efficiently. Our culture is rooted in innovation, encouraging our team to stay curious and engaged. By joining us, you become part of a global, remote/hybrid-friendly team dedicated to bridging health care gaps with a solid sense of social responsibility. At Episource, we are enriching lives, including those of our team members through Caring. Connecting. Growing together. \n",
    "\n",
    "As a Data Analyst, you will play a crucial role in transforming raw data into actionable insights that drive business decisions and strategies. Your responsibilities will include data collection, analysis, interpretation, and transformation, helping stakeholders understand complex datasets to make informed choices and optimize performance\n",
    "\n",
    "The data analyst will execute data pipelines, analyze transformed data to validate its completeness and gather insight to impact business decisions. Additionally, the data analysis will handle ad hoc analysis request-related data surrounding medical Record retrieval to assist in determining the location of medical charts.\n",
    "\n",
    "You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.\n",
    "\n",
    "Primary Responsibilities\n",
    "\n",
    "    Collect, clean, and preprocess data from various sources to ensure accuracy and consistency\n",
    "    Identify and address data quality issues, outliers, and missing values\n",
    "    Perform exploratory data analysis to uncover trends, patterns, and relationships within datasets\n",
    "    Oversee ingestion pipelines and troubleshoots errors related to data. Resolve errors using approved data cleaning techniques\n",
    "    Translate complex data analysis into clear and actionable insights for stakeholders\n",
    "    Collaborate with cross-functional teams to identify opportunities for process optimization and strategic decision-making\n",
    "    Build analytics tools that utilize the data pipeline to provide actionable insights into operational efficiency, and other critical business performance metrics\n",
    "    Develops and maintains solid relationships with all internal and external stakeholders\n",
    "    Document analysis methodologies, assumptions, and findings for future reference and replication\n",
    "    Requires an individual to maintain the ability to work in an environment with PHI / PII data \n",
    "\n",
    "You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.\n",
    "\n",
    "Required Qualifications\n",
    "\n",
    "    2+ years of experience in data analytics\n",
    "    2+ years of experience writing intermediate to advanced SQL (joining many tables, SQL functions, CTE's)\n",
    "    Experience working with Python (Ability to read and understand Python scripts and ability to make adjustments to scripts)\n",
    "    Experience with data visualization tools such as Tableau \n",
    "    Proven knowledge of data analysis and data quality best practices\n",
    "    Demonstrated excellent verbal and written communication skills, including ability to effectively communicate with internal and external customers\n",
    "    Proficient in Excel (v-look ups, pivot tables)\n",
    "    Ability and willingness to work occasional weekends and/or evening work\n",
    "\n",
    "Preferred Qualifications\n",
    "\n",
    "    Proven knowledge of healthcare data including member, claims, and provider data\n",
    "    Proven knowledge of industry standards and practices related to medical records retrieval\n",
    "    Demonstrated understanding of data pipelines, database structures and ETL practices \n",
    "    All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\reddy\\\\Downloads\\\\LLM_Project\\\\experiments\\\\skills'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../resume.json') as f:\n",
    "    resume_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_skills = resume_json['skills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "import datetime\n",
    "import subprocess\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('GROQ_API_KEY')\n",
    "client = Groq(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "def analyze_skills(candidate_profile: dict, job_description, max_retries=5) -> None:\n",
    "    # Step 1: Define prompt for the first LLM (Skills and Responsibilities Optimization)\n",
    "    first_prompt = f\"\"\"\n",
    "    You are a resume optimization expert for ATS compliance. Your task is to enhance the candidate's skills and modify it based on the job description:\n",
    "\n",
    "    1. **Extract Key Skills and Responsibilities**:\n",
    "       - Identify and list the key skills, responsibilities, and keywords from the job description that are essential for the ideal candidate.\n",
    "\n",
    "    2. **Optimize the Candidate's Skills Section**:\n",
    "       - Review the candidate's \"Skills\" section carefully.\n",
    "       - Prioritize and rearrange skills based on relevance to the job description.\n",
    "       - Remove irrelevant skills and avoid adding new or highly technical skills the candidate doesn't possess.\n",
    "       - Add transferable, miscellaneous, analytical or any other skill section if applicable, ensuring they align with the job description and are somewhat consistent with the candidate’s profile.\n",
    "       - Do not include any sections (e.g., \"Cloud\", \"Programming Languages\") if they have no corresponding skills or if adding them would result in an empty or visually redundant section. Remove the section entirely if empty.\n",
    "       - You may add new sections only if they are relevant and include appropriate skills.\n",
    "       - Return the output in JSON format, with each section containing only non-empty skill lists.\n",
    "\n",
    "    3. **Output Format**:\n",
    "       - Provide the optimized \"Skills\" section in JSON format.\n",
    "\n",
    "    ### Candidate Profile:\n",
    "    {json.dumps(candidate_profile, indent=4)}\n",
    "\n",
    "    ### Job Description:\n",
    "    {json.dumps(job_description, indent=4)}\n",
    "\n",
    "    Your output should be in the form of an enhanced JSON object for the \"Skills\" section.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Step 2: Query the LLM for the optimized skills section\n",
    "        first_response = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[{\"role\": \"user\", \"content\": first_prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=2048,\n",
    "            top_p=0.9,\n",
    "            stream=False,\n",
    "            stop=None,\n",
    "        )\n",
    "        first_output = first_response.choices[0].message.content if first_response.choices else None\n",
    "\n",
    "        if not first_output:\n",
    "            print(\"First LLM did not produce a valid response.\")\n",
    "            return\n",
    "\n",
    "        # Save the first output for review\n",
    "        with open(\"first_output.txt\", \"w\") as file:\n",
    "            file.write(first_output)\n",
    "        print(\"First LLM output saved to 'first_output.txt'.\")\n",
    "\n",
    "        # Step 3: Retry parsing and saving JSON output up to max_retries times\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            print(f\"Attempt {attempt} of {max_retries}...\")\n",
    "\n",
    "            second_prompt = f\"\"\"\n",
    "            Extract and return only the JSON object from the following text. Do not include any extra explanations or comments.\n",
    "\n",
    "            ### Input:\n",
    "            {first_output}\n",
    "\n",
    "            ### Output:\n",
    "            Provide only the valid JSON object.\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                second_response = client.chat.completions.create(\n",
    "                    model=\"llama3-70b-8192\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": second_prompt}],\n",
    "                    temperature=0.3,\n",
    "                    max_tokens=1024,\n",
    "                    top_p=0.9,\n",
    "                    stream=False,\n",
    "                    stop=None,\n",
    "                )\n",
    "                second_output = second_response.choices[0].message.content if second_response.choices else None\n",
    "\n",
    "                if not second_output:\n",
    "                    print(\"Second LLM did not produce a valid response.\")\n",
    "                    continue\n",
    "\n",
    "                # Validate the JSON output\n",
    "                try:\n",
    "                    extracted_json = json.loads(second_output.strip())\n",
    "                    # Save the JSON file\n",
    "                    with open(\"skills.json\", \"w\") as file:\n",
    "                        json.dump(extracted_json, file, indent=4)\n",
    "                    print(\"Optimized skills JSON saved to 'skills.json'.\")\n",
    "                    return  # Exit loop if successful\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Error parsing JSON from the second LLM response.\")\n",
    "                    print(f\"Invalid JSON output: {second_output}\")\n",
    "                    # Continue to the next attempt\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during LLM processing on attempt {attempt}: {e}\")\n",
    "\n",
    "            # Optional: Delay between retries\n",
    "            time.sleep(1)\n",
    "\n",
    "        print(f\"Failed to extract valid JSON after {max_retries} attempts.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM processing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First LLM output saved to 'first_output.txt'.\n",
      "Attempt 1 of 5...\n",
      "Optimized skills JSON saved to 'skills.json'.\n"
     ]
    }
   ],
   "source": [
    "analyze_skills(candidate_skills, job_description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
