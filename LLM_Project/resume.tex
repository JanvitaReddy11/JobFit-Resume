
    \documentclass{resume}

    \begin{document}
    \name{Janvita Reddy}
\details{979-709-1497} {reddyjanvita11@gmail.com} {linkedin.com/in/janvita/}{github.com/JanvitaReddy11}
\begin{rSection}{Education}

\begin{rSubsectionedu}{Texas A\&M University}{College Station, TX, USA}{Aug 23 - May 25}{MS in Data Science}{Data Science}{4.0/4.0}

\end{rSubsectionedu}
\vspace{-15pt}

\begin{rSubsectionedu}{Sardar Vallabhbhai National Institute of Technology}{Surat, India}{Aug 19 - May 23}{B.Tech in Mechanical Engineering}{Mechanical Engineering}{9.21/10.0}

\end{rSubsectionedu}
\vspace{-15pt}
\end{rSection}
\begin{rSection}{Skills}
\begin{itemize}[left=0pt, label={}, align=parleft]
\vspace{-5pt}
\item [] \textbf{Programming languages}: {Python}, {R}, {SQL}
\vspace{-5pt}
\item [] \textbf{Ml libraries}: {NumPy}, {Pandas}, {Pytorch}, {Keras}, {OpenCV}, {TensorFlow}, {Scikit}
\vspace{-5pt}
\item [] \textbf{Models}: {Regression}, {SVM}, {KNN}, {Decision Trees}, {Neural Networks}, {Transformers}
\vspace{-5pt}
\item [] \textbf{Cloud products}: {GCP Compute Engine}, {Cloud storage}, {Vertex AI}, {BigQuery}, {AWS EC2}, {AWS S3}
\vspace{-5pt}
\item [] \textbf{Data technology}: {Hadoop}, {PySpark}, {Docker}, {Airflow}, {Git}, {MongoDB}, {Tableau}, {PowerBI}
\vspace{-5pt}
\item [] \textbf{Analytical skills}: {Data preprocessing}, {Feature engineering}, {Hyperparameter tuning}, {Model evaluation}
\vspace{-5pt}
\item [] \textbf{Transferable skills}: {Automation}, {Data ingestion}, {Pipeline tracking}, {Deployment}, {CI/CD pipelines}
\end{itemize}
\end{rSection}
\begin{rSection}{Work Experience}
\vspace{-3pt}

\begin{rSubsectionWork}{Graduate Research Assistant}{Texas A\&M University}{USA}{Jan 24 - Present}

\vspace{-3pt}
\item Annotated 1800+ images using LabelMe software integrated with SAM to identify cotton balls, enhancing model performance by 0.70 IoU score.
\vspace{-3pt}
\item Engineered segmentation models like Unet attention, CBAM, Pix2Pix Unet, and SwinUnet, achieving a 0.70 IoU score and improving model performance by implementing LoRA and skip connections to finetune SAM.
\vspace{-3pt}
\item Conducted correlation analysis between pixels and cotton yield, achieving a 0.91 correlation accuracy and identifying key factors influencing crop yield.
\end{rSubsectionWork}
\vspace{-3pt}

\begin{rSubsectionWork}{Graduate Research Assistant}{Texas A\&M University}{USA}{Sep 23 - Dec 23}

\vspace{-3pt}
\item Experimented with CNN models like AlexNet, VGG-16, ResNet, EfficientNet, and 3D CNN for classification of stress-induced crops, conducting hyperparameter tuning using Keras Tuner and achieving a 6\% accuracy improvement on test data with an F1 score of 0.97.
\vspace{-3pt}
\item Implemented LSTM model on time series images of cotton crops to capture spatio-temporal relations, resulting in a 6\% accuracy improvement on test data with an F1 score of 0.97.
\end{rSubsectionWork}
\vspace{-3pt}

\begin{rSubsectionWork}{Research Intern}{IISER Bhopal}{India}{May 22 - July 22}

\vspace{-3pt}
\item Designed a domain generalization model to improve resilience across multiple domains, utilizing adversarial learning for model adaptation and integrating Grad-CAM for better interpretability.
\vspace{-3pt}
\item Applied K-means clustering to segregate domains and utilized AlexNet as a feature extractor, achieving a 3\% increase in accuracy over the baseline model by incorporating multi-domain discriminators.
\end{rSubsectionWork}
\vspace{-3pt}

\begin{rSubsectionWork}{Research Intern}{IIT Kharagpur}{India}{May 21 - Feb 22}

\vspace{-3pt}
\item Automated the detection of weld path by developing a machine learning algorithm for robotic welding, implementing YOLOv5 to detect joints with a precision of 99.5\%.
\vspace{-3pt}
\item Leveraged image processing techniques to denoise and identify edges, yielding an absolute error of Â±1mm for weld lines and Â±0.1mm for gaps.
\end{rSubsectionWork}
\end{rSection}
\begin{rSection}{Projects}
\vspace{-3pt}

\begin{rSubsectionProj}{AI Assistant}

\vspace{-3pt}
\item Developed a personal AI assistant that automates tasks like sending emails, scheduling meetings, performing internet searches, and answering queries from PDFs, leveraging RAG for multi-document querying and context-aware responses.
\vspace{-3pt}
\item Integrated specialized agents into a unified master agent using ReAct framework for seamless task execution, ensuring efficient and accurate responses.
\end{rSubsectionProj}
\vspace{-3pt}

\begin{rSubsectionProj}{Reproducing ChatGPT}

\vspace{-3pt}
\item Trained GPT-2 124M model from scratch on FineWeb dataset, achieving a 6.5\% improvement in HellaSwag accuracy from 0.31 to 0.33 through enhancements including replacing LayerNorm with RMSNorm and incorporating Rotary Positional Encodings.
\vspace{-3pt}
\item Optimized multi-head attention by implementing Group Query Attention, resulting in enhanced model performance.
\end{rSubsectionProj}
\vspace{-3pt}

\begin{rSubsectionProj}{Job Recommendation System}

\vspace{-3pt}
\item Designed a personalized job recommendation system by analyzing users' work history and applying user collaborative filtering, resulting in 89\% accuracy and a hit rate of 0.78.
\vspace{-3pt}
\item Conducted text preprocessing to transform job descriptions and user profiles into word2vec embeddings and TF-IDF vectors, ensuring accurate matching.
\end{rSubsectionProj}
\vspace{-3pt}

\begin{rSubsectionProj}{Wide Residual Attention Networks}

\vspace{-3pt}
\item Enhanced ResNet model by adding mask attention after residual blocks, achieving 95.4\% accuracy on the CIFAR-10 dataset through applied normalization, random flipping, and Gaussian noise to training images.
\vspace{-3pt}
\item Improved model performance by incorporating attention mechanisms, enabling the model to focus on relevant features.
\end{rSubsectionProj}
\vspace{-3pt}

\begin{rSubsectionProj}{Chicken Disease Classification using MLOps}

\vspace{-3pt}
\item Designed a scalable ML solution by integrating pre-trained VGG16 with custom layers, executing data ingestion, training, and evaluation using DVC for pipeline tracking.
\vspace{-3pt}
\item Deployed the model using Docker on AWS EC2, leveraging GitHub Actions for automated CI/CD pipelines, ensuring efficient model deployment and management.
\end{rSubsectionProj}
\vspace{-3pt}

\begin{rSubsectionProj}{Fraud Classification using GCP}

\vspace{-3pt}
\item Collected and stored employee data in GCS bucket, transforming data and masking sensitive information using Cloud Data Fusion.
\vspace{-3pt}
\item Created dashboards using Looker and automated ETL process with Airflow, enabling real-time data analysis and visualization.
\end{rSubsectionProj}
\end{rSection}
\end{document}